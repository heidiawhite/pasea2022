{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNq4HgykhnJK"
      },
      "source": [
        "# Part 5 - Galaxy Classification Using Convolutional Neural Networks\n",
        "\n",
        "In today's Day 5 tutorial you will be working through an example in which we will be classifying the morphology (i.e. the physical shape) of galaxies in the Sloan Digital Sky Survey (SDSS) using a convolutional neural network named GaMorNet.\n",
        "\n",
        "Note: The procedural portion of this tutorial is largely based on the GaMorNet tutorials available as part of the GaMorNet documentation (https://gamornet.readthedocs.io/en/latest/tutorials.html). All mentions of \"the paper\" in this tutorial, refer to [Ghosh et. al. (2020)](https://iopscience.iop.org/article/10.3847/1538-4357/ab8a47).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjBuCe8G3HBh"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Galaxies in the Universe can take on a wide variety of shapes and sizes. Galaxy morphological classification, or classifying galaxies by their shape and visual appearance, is a system used by astronomers to divide galaxies into groups. Early observations of a few hundred nearby, bright galaxies by Edwin Hubble in the 1920s led to the development of what is known as the “Hubble Sequence”, a classification scheme that identifies three main morphological types: spirals, lenticulars, and ellipticals (Hubble 1926; Sandage 1961). Although galaxies can be classified by eye, the advent of large astronomical surveys such as the SDSS has ushered in an era of big datasets and modern efforts to classify galaxies by shape largely use computational methods.\n",
        "\n",
        "### The Shape of Galaxies in the Universe\n",
        "\n",
        "<img src=\"https://github.com/heidiawhite/pasea-arp/blob/main/PythonML_workshop/Part_5/imgs/main_image_deep_field_smacs0723-5mb.jpg?raw=1\" alt=\"Alternative text\" />\n",
        "\n",
        "The above image was recently produced by the James Webb Space Telescope (JWST; NASA). This first Deep Field image features galaxy cluster SMACS 0723, and it is teeming with thousands of galaxies – including the faintest objects ever observed at infrared wavelengths. JWST’s image took roughly 12.5 hrs of telescope time to create and covers a tiny patch of sky (approximately the size of a grain of sand held at arm’s length by someone on the ground) and reveals thousands of galaxies of all shapes and sizes in just a sliver of the vast universe. This image is an excellent example of the wonderfully diverse spectrum of galaxy morphology in the Universe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1lyIq05mkq3"
      },
      "source": [
        "### `GaMorNet`\n",
        "\n",
        "In the tutorial below we will be using The Galaxy Morphology Network (GaMorNet). GaMorNet is a convolutional neural network that can classify galaxies as being \"disk-like\", \"elliptical-like\" or \"indeterminate\" based on how light is distributed within the galaxy. The advantage of GaMorNet is that it doesn’t need a large amount of training data to provide reliable predictions and can work across different types of data sets.\n",
        "\n",
        "Although GaMorNet is a far more sophisticated neural network than the simple example we developed to classify flowers, the basic principles remain the same. Here, GaMorNet assesses galaxy shapes by using physically relevant property of their **bulge-to-total** luminosity ratio. \n",
        "\n",
        " - Galaxies with light mainly distributed in disk structures are said to be **disk-dominated**. This is the case in, for example, spiral galaxies like our own Milky Way.\n",
        "\n",
        "- Galaxies with light primarily in large, round bulge structures are said to be **bulge-dominated**. Examples of this include massive elliptical galaxies like Messier 87 (also known as Virgo A or NGC 4486).\n",
        "\n",
        "- Galaxies where it's not entirely clear which component dominates (or in cases where the algorithm is unable to make a classification are said to be **indeterminate**.\n",
        "\n",
        "<img src=\"https://github.com/heidiawhite/pasea-arp/blob/main/PythonML_workshop/Part_5/imgs/galaxyzoo_spiral_elliptical.jpeg?raw=1\" alt=\"Alternative text\" />\n",
        "\n",
        "The above image shows an example of disk vs. elliptical type galaxy.\n",
        "\n",
        "GaMorNet assesses the distribution of light within each galaxy in an image and reports back a likelihood prediction that the galaxy is bulge-dominated, disk-dominated, or indeterminate. Because of this, each galaxy receives *three* likelihood predictions, which each range between 0 and 1.\n",
        "\n",
        "### Running `GaMorNet` in Google Colab\n",
        "\n",
        "Although this tutorial can be run on any machine which has GaMorNet installed, it's pretty handy to run this on Google Colab as you can easily use Colab's GPUs for this tutorial.\n",
        "\n",
        "Note that with the free version of CoLab, you will only have access to a limited amount of memory. Thus, the number of images we use here for training/testing our network is very small in comparison to how many would be used in astronomy research. In reality, GaMorNet can handle hundreds of thousands of images! \n",
        "\n",
        "This first section is meant to be run only when following this tutorial in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2wSGVu4ihqa"
      },
      "source": [
        "### Make things Fast! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWvqtYGaijke"
      },
      "source": [
        "Before we dive in, let's make sure we're using a Google CoLab GPU for this tutorial.\n",
        "\n",
        "To do this, go to the menus at the top and select \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" -> \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUDD3Qke7Ws_"
      },
      "source": [
        "### Installing Libraries Needed for this Tutorial\n",
        "\n",
        "Let's first begin by using some *pip* commands to install some required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBSCZfMM7WJV"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install astropy\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en4lZVmont-b"
      },
      "source": [
        "### Install GaMorNet\n",
        "\n",
        "Next, let's install the GaMorNet software.\n",
        "\n",
        "Note: This command make take a few minutes to run!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.version"
      ],
      "metadata": {
        "id": "2p4z4iQBrN_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oct 11, 2022\n",
        "# tensorflow._api.v1.version works with 2.3\n",
        "# tensorflow v2 works with 2.2\n",
        "# Gamornet requirements:\n",
        "# tensorflow-gpu ~=1.13\"\n",
        "# tflearn ~=0.3\"\n",
        "# keras ~=2.2,<2.4\"\n",
        "!pip install keras==2.2"
      ],
      "metadata": {
        "id": "F7Lv7434vat8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "id": "IT7tB8rOFJdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbBNuljhhnyf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Suppressing TF warnings and info for a cleaner environ\n",
        "# Set this to 0,1 for info and warnings respectively.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "!pip install -q --upgrade gamornet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYsnfr7tj3D-"
      },
      "outputs": [],
      "source": [
        "##Checking which version of Tensorflow & GaMorNet is being used and whether the installation worked.\n",
        "import gamornet\n",
        "print(tf.__version__)\n",
        "print(gamornet.__version__)\n",
        "from gamornet.keras_module import gamornet_train_keras, gamornet_tl_keras, gamornet_predict_keras\n",
        "from gamornet.tflearn_module import gamornet_train_tflearn, gamornet_tl_tflearn, gamornet_predict_tflearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svdqWX2sE7xw"
      },
      "source": [
        "\n",
        "The following snippet will verify that we have access to a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF9QiuyuE7ih"
      },
      "outputs": [],
      "source": [
        "#Checking access to GPU\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZdvFR_IgTmn"
      },
      "source": [
        "## Training with `GaMorNet`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0MNIlNUSZJ8"
      },
      "source": [
        "GaMorNet can quite easily be trained from scratch using images. \n",
        "\n",
        "In this demonstration, we will use 90 *simulated* SDSS images for the purposes of training and 10 *simulated* SDSS images for validation. All these simulated images come from the set of simulated galaxies created for the paper referenced above. Note: these are not images of actual galaxies. They have been created using software to mimic the wide variety of galaxies observed in the wild.\n",
        "\n",
        "All these images contain disk + bulge components. As described in the paper, Ghosh et al. (2020) has also convolved these simulations with a representative point-spread-function and added representative noise to better mimic actual scientific data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwzpM9YMUPqn"
      },
      "source": [
        "### Downloading the Data\n",
        "\n",
        "First, let's download the images that we are going to use to train GaMorNet. We will download these into the local filesystem from Yale Astronomy's FTP service, where these are hosted.\n",
        "\n",
        "We are going to download all the 100 images (90+10) as a single archive and then export it to a single folder called `training_imgs`. The images are in the FITS format and are named `output_img_xx.fits` where xx runs from 0 to 99.\n",
        "\n",
        "We are also going to download the `sim_para.txt` file containing the ground-truth parameters for the above galaxies. Using these values, we are going to calculate the bulge-to-total light ratio of each galaxy and determine the labels to be used during the training process. \n",
        "\n",
        "\n",
        "*Tip: The `%%bash` command lets Colab know that all the commands in this shell needs to be passed the local unix virtual environment.*\n",
        "\n",
        "*Tip: To view the files in use on Colab, click the folder icon on the left sidebar.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TThUIR8Y4lc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#get zip and txt file from server\n",
        "wget ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/train_images/training_imgs.tar.gz\n",
        "wget ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/train_images/sim_para.txt\n",
        "\n",
        "#Unzip the Archive\n",
        "tar -xvf training_imgs.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikheBSGd5PqF"
      },
      "source": [
        "In the cell below, write a code block which visualizes two of the training set FITS files and displays them side by side.\n",
        "\n",
        "Note: to do this you may want to use `subplots` in `matplotlib`. The documentation on its functionality can be found [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWj6MLH55dQC"
      },
      "outputs": [],
      "source": [
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWNc2QMWBXCN"
      },
      "source": [
        "***\n",
        "\n",
        "#### Q - Iterate through visualizing some of the training data images. How well do they resemble real galaxies? Why might astronomers choose to use simulated images as training data?\n",
        "\n",
        "***\n",
        "\n",
        "Ans. Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAwLO4hk-C3Q"
      },
      "source": [
        "### Preparing the Data\n",
        "\n",
        "In this section, we will generate the training and validation image arrays as well as the corresponding labels to be used during the networks' training process.\n",
        "\n",
        "\n",
        "First, let's read in the `.txt` file and calculate the difference in disk and bulge magnitudes (i.e. brightnesses) for each of the galaxies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex46Lm2c_3I8"
      },
      "outputs": [],
      "source": [
        "import pylab as plt\n",
        "\n",
        "#Let's read in the sim_para.txt file \n",
        "gal_para = plt.genfromtxt(\"./sim_para.txt\",names=True,usecols = (4,11))\n",
        " \n",
        "#difference b/w the integrated magnitude of the disk and bulge components. \n",
        "#The rows in the file and thus the elements in the array below correspond to\n",
        "#the numbers in the names of the image files. (i.e. the 0th element corresponds\n",
        "#to output_img_0.fits)\n",
        "disk_bulge_mag = gal_para[\"Inte_Mag\"] - gal_para[\"Inte_Mag_2\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pDautJ1CWTy"
      },
      "source": [
        "Next, let's define two convenience functions, which will assist us in creating the image and label arrays. The first reshapes each data array to add a third dimension. The second function adds a vector that acts as a label for which type of galaxy the array represents. This label is determined by using the magnitudes calculated in the cell above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWcg5PBi-RSL"
      },
      "outputs": [],
      "source": [
        "# Convenience Function to get and return images as numpy arrays\n",
        "\n",
        "def image_handler(i):\n",
        "  return np.reshape(fits.getdata(\"./train_images/output_img_\"+str(i)+\".fits\",\n",
        "                                 memmap=False),newshape=(167,167,1)) \n",
        "  #We use the reshape command just to add the extra 3rd dimension. The image is \n",
        "  #originally 167*167. So, in essence no re-sizing is taking place in the X or Y\n",
        "  #directions.\n",
        "\n",
        "\n",
        "# Convenience Function to get and return the training labels of each galaxy\n",
        "# in the one-hot encoding format. i.e. disk-dominated galaxies will be represented\n",
        "# by the array [1,0,0], bulge-dominated by [0,0,1] and indeterminate by [0,1,0]\n",
        "\n",
        "def label_handler(i):\n",
        "  \n",
        "  target_vect = [0]*3\n",
        "\n",
        "  if (disk_bulge_mag[i] < -0.22): #  (Lb/LT) < 0.45\n",
        "    target_vect[0] = 1  #disk-dominated       \n",
        "  \n",
        "  elif ( -0.22 <=  disk_bulge_mag[i] <= 0.22):\n",
        "      target_vect[1] = 1 #indeterminate\n",
        "  \n",
        "  else: #  (Lb/LT) > 0.55\n",
        "      target_vect[2] = 1 #bulge-dominated\n",
        "\n",
        "  return target_vect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQesQrpK-Pz2"
      },
      "source": [
        "Now, we are going to use the first 90 images to create the training set and the last 10 to create the validation set. We are multi-threading the process below -- although this is an absolute overkill for 100 images, it's very handy while dealing with large numbers of images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_SCF8wWDiSD"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "from astropy.io import fits\n",
        "\n",
        "NUM_THREADS = 2\n",
        "\n",
        "pl = Pool(NUM_THREADS)\n",
        "training_imgs = np.array(pl.map(image_handler,range(0,90)))\n",
        "training_labels = np.array(pl.map(label_handler,range(0,90)))\n",
        "\n",
        "valdiation_imgs = np.array(pl.map(image_handler,range(90,100)))\n",
        "validation_labels = np.array(pl.map(label_handler,range(90,100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNG1X-fSR8oC"
      },
      "source": [
        "There are two modules available for training GaMorNet:\n",
        "\n",
        " - `Keres`\n",
        " - `TFLearn`\n",
        "\n",
        " These two models function somewhat similarly, and differ primarily in terms of their individual efficacies in classifying bulge and disk galaxies. More info on these two approaches can be explored here: https://gamornet.readthedocs.io/en/latest/usage_guide.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw-l4YKMbLdK"
      },
      "source": [
        "## Training GaMorNet using Keras\n",
        "\n",
        "Now, we will be using the images and the labels generated above to train GaMorNet. Explore the documentation for `Keras` to learn about what each of these arguments represent in the `gamornet_train_keras` function:\n",
        "\n",
        "https://gamornet.readthedocs.io/en/latest/api_docs.html#gamornet.keras_module.gamornet_train_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBTLm74QbMnG"
      },
      "outputs": [],
      "source": [
        "from gamornet.keras_module import gamornet_train_keras\n",
        "\n",
        "model = gamornet_train_keras(training_imgs,training_labels,valdiation_imgs,\n",
        "                             validation_labels,input_shape='SDSS', epochs=50, \n",
        "                             checkpoint_freq=25, batch_size=64, lr=0.0001, \n",
        "                             loss='categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbzt1qHGG7ug"
      },
      "source": [
        "The above command trains a model using the images we prepared for 50 epochs using a learning rate of 0.0001 and a categorical cross-entropy loss function. The `checkpoint_freq = 25` parameter also ensures that every 25 epochs, a snapshot of the model is saved. These models are named as `model_x.hdf5` where x refers to the epoch at which the model was saved. The `input_shape` parameter specifies the shape of the input images. Setting this to `SDSS` automatically sets the value to `(167,167,1)`\n",
        "\n",
        "For an explanation of the different input parameters of `gamornet_train_keras`, please have a look at the [API documentation](https://gamornet.readthedocs.io/en/latest/api_docs.html).\n",
        "\n",
        "In the output above, the `accuracy` and `loss` refer to the metrics calculated on the training set at the end of each epoch while `val_loss` and `val_accuracy` refer to the metrics calculated on the validation data.\n",
        "\n",
        "It's really important to understand the `loss` and `accuracy` output in the context of machine learning.\n",
        "\n",
        " - A **loss function** is used to optimize a machine learning algorithm. The \"loss\" is calculated on training and validation and its interpretation is based on how well the model is doing in evaluating these two sets. The \"loss\" is the sum of errors made for each example in training or validation sets. Loss value implies how poorly or well a model behaves after each iteration of optimization.\n",
        "\n",
        " - An **accuracy metric** is used to measure the algorithm’s performance in an interpretable way. The accuracy of a model is usually determined after the model parameters and is calculated in the form of a percentage. It is the measure of how accurate your model's prediction is compared to the true data. For example, suppose you have 1000 test samples. If your model is able to classify 990 of them correctly, then the model’s accuracy will be 99.0%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYm62rwTjEfs"
      },
      "source": [
        "***\n",
        "\n",
        "#### Q - Read through the training output from `Keras` above. How do the loss and accuracy values evolve over time as the training proceeds? Provide an interpretation.\n",
        "\n",
        "***\n",
        "\n",
        "Ans. Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhQgRdXJtwE"
      },
      "source": [
        "Congrats! You have trained your first GaMorNet model!! You can have a look at the model's structure using the command below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fs124brJpVI"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkVOkDibLMmD"
      },
      "source": [
        "**Important:**\n",
        "The above process also generates a `metrics.csv` file, which contains the loss and accuracy calculated on the validation as well as the training data. \n",
        "\n",
        "We highly recommend using the data in this file to check how the loss and accuracies vary with training. This is extremely helpful in judging whether the model was trained properly and sufficiently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKTEFpyHpHY"
      },
      "source": [
        "## Training `GaMorNet` using TFLearn\n",
        "\n",
        "Now, we will be using the images and the labels generated above to train GaMorNet using `TFLearn`.\n",
        "\n",
        "Read up on the the documentation for this module [here](https://gamornet.readthedocs.io/en/latest/api_docs.html#module-gamornet.tflearn_module)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUP-k-NtHpHa"
      },
      "outputs": [],
      "source": [
        "from gamornet.tflearn_module import gamornet_train_tflearn\n",
        "\n",
        "model = gamornet_train_tflearn(training_imgs,training_labels,valdiation_imgs,\n",
        "                             validation_labels,input_shape='SDSS', epochs=20, \n",
        "                             max_checkpoints=2, batch_size=64, lr=0.0001, \n",
        "                             loss='categorical_crossentropy',clear_session=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9StaWVvHpHe"
      },
      "source": [
        "The above command trains a model using the images we prepared for 20 epochs (or defined periods of time) using a learning rate of 0.0001 and a categorical cross-entropy loss function. The `max_checkpoints = 2` parameter ensures that the latest 2 snapshots of the epochs will always be saved during training. Three files are saved for each snapshot and the naming format of the checkpoints is `check-x.data`,`check-x.index`,`check-x.meta` where x refers to the step number at which the model was saved. The `input_shape` parameter specifies the shape of the input images. Setting this to `SDSS` automatically sets the value to `(167,167,1)`. \n",
        "\n",
        "In the output above, the `acc` and `loss` refer to the accuracy and loss calculated on the training set at the end of each epoch while `val_loss` and `val_acc` refer to the metrics calculated on the validation data. \n",
        "\n",
        "The `clear_session = True` parameter value instructs GaMorNet to clear the TensorFlow graphs created earlier. We highly recommend setting `clear_session` to `True` in notebooks while using the `tflearn_module` as otherwise it might fail. \n",
        "\n",
        "For an explanation of the different input parameters of `gamornet_train_tflearn`, please have a look at the [API documentation](https://gamornet.readthedocs.io/en/latest/api_docs.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSoVi9LRMhUv"
      },
      "source": [
        "**Tip:** Unlike with the keras module, the tflearn module doesn't automatically save the metrics. Instead you have to redirect the Python output generated to a file in order to keep track of the metrics. \n",
        "\n",
        "When running some python script this can be done simply using `python script.py > out.txt`. This will save all the screen output in `out.txt`.\n",
        "\n",
        "Thereafter the following snippet of Python Code can easily search for the relevant metrics in the screen output file. \n",
        "\n",
        "```python\n",
        "###################################\n",
        "# accParser.py\n",
        "#\n",
        "# Takes tflearn screen output and extracts loss, acc and val_acc every epoch for visualization\n",
        "####################################\n",
        "import sys\n",
        "\n",
        "if (len(sys.argv) != 2):\n",
        "        print \"Exiting Program....\\nUsage: python accParser.py /path/to/screen/output\"\n",
        "\n",
        "\n",
        "dataPath = sys.argv[1] #the first argument is the path to the screen grab of the TF Learn run\n",
        "\n",
        "dataFile = open(dataPath, 'r')\n",
        "outFile = open(dataPath[:-6] + 'out.txt', 'w')\n",
        "\n",
        "outFile.write(\"epoch loss acc val_acc\\n\")\n",
        "resultLines = dataFile.readlines()\n",
        "\n",
        "for line in resultLines:\n",
        "        if 'val_acc' in line:\n",
        "                words = line.split()\n",
        "\n",
        "                #validation step\n",
        "                if words[-2:-1] != ['iter:']:\n",
        "                        print \"Something doesn't look right. Skipping an occurene of val_acc\"\n",
        "                        continue\n",
        "\n",
        "                outFile.write(words[words.index(\"epoch:\")+1] + \" \")\n",
        "                outFile.write(words[words.index(\"loss:\")+1] + \" \")\n",
        "                outFile.write(words[words.index(\"acc:\")+1] + \" \")\n",
        "                outFile.write(words[words.index(\"val_acc:\")+1] + \"\\n\")\n",
        "\n",
        "dataFile.close()\n",
        "outFile.close()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CI4QJAROoGo"
      },
      "source": [
        "**Important:** We highly recommend checking how the loss and accuracies vary with training. This is extremely helpful in judging whether the model was trained properly and sufficiently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Trd52HQV7xU"
      },
      "source": [
        "### Summary & Takeaways\n",
        "\n",
        "* `gamornet_train_keras` and `gamornet_train_tflearn` are the two functions that can be used to train GaMorNet models. \n",
        "\n",
        "* For understanding the differences between the Keras and TFLearn modules, please refer to the [PDR Handbook](https://gamornet.readthedocs.io/en/latest/usage_guide.html). \n",
        "\n",
        "* The [PDR Handbook](https://gamornet.readthedocs.io/en/latest/usage_guide.html) also contains advice about which situations warrant the training of models from scratch and in which cases you can use the models which we have released. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeTXxzWdXuAA"
      },
      "source": [
        "# Making Morphological Classification Predictions via `GaMorNet`\n",
        "\n",
        "Let's now used our trained neural network to start classifying galaxies in SDSS. For the purposes of this demonstration, we are going to use the models trained on SDSS simulations and real data in the paper. We are going to perform the prediction on two SDSS g-band images from our testing dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe2iJIlUYkJm"
      },
      "source": [
        "## Downloading & Visualizing the Data\n",
        "\n",
        "First, let's download the images that we are going to use to perform the predictive analysis. We will download these into the local filesystem from Yale Astronomy's FTP service, where these are hosted.\n",
        "\n",
        "\n",
        "*Tip: The `%%bash` command lets Colab know that all the commands in this shell needs to be passed the local unix virtual environment.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-5Ivrh9VrEW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#get zip file from server\n",
        "wget ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/predict_images/predict_images.tar.gz\n",
        "\n",
        "#create new directory and unzip to it\n",
        "mkdir predict_images\n",
        "tar -xvf predict_images.tar.gz -C ./predict_images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X87eSs7NY622"
      },
      "source": [
        "Now, let's take a quick look at these two images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB9Vv391Yt-X"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "plt.figure(dpi=120)\n",
        "plt.subplot(1,2,1) #setting up 1st subplot\n",
        "\n",
        "#reading in the data and plotting the first image\n",
        "img_1_data = fits.getdata(\"./predict_images/587722984439545906-g.fits\")\n",
        "plt.imshow(img_1_data)\n",
        "\n",
        "plt.subplot(1,2,2) #setting up the 2nd subplot\n",
        "\n",
        "#reading in the data and plotting the second image\n",
        "img_2_data = fits.getdata(\"./predict_images/587725552281976904-g.fits\")\n",
        "plt.imshow(img_2_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFzzwV6Qbo88"
      },
      "source": [
        "## Performing Predictions with `Keras`\n",
        "\n",
        "Now, we will be using the models trained on SDSS simulations and real data from the paper in order to perform predictions on the two above images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c99RjJqYbsPn"
      },
      "outputs": [],
      "source": [
        "from gamornet.keras_module import gamornet_predict_keras\n",
        "\n",
        "#Adding an explicit third axis to the image data (as this is needed for\n",
        "#the GaMorNet function to work properly). Just to be clear the images\n",
        "#were already 167*167, so we are not resizing the X,Y dimensions in any\n",
        "#way\n",
        "img_1_data = np.reshape(img_1_data,newshape=(167,167,1))\n",
        "img_2_data = np.reshape(img_2_data,newshape=(167,167,1))\n",
        "img_array = np.array([img_1_data,img_2_data])\n",
        "\n",
        "#Performing Predictions\n",
        "predictions = gamornet_predict_keras(img_array, model_load_path='SDSS_tl', input_shape='SDSS', batch_size=64)\n",
        "\n",
        "print(\"COMPLETE!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em4hl8hvb7hw"
      },
      "source": [
        "The `model_load_path = 'SDSS_tl'` is a special parameter value which automatically downloads and uses the final SDSS model of the paper. The `input_shape` parameter specifies the shape of the input images. Setting this to `SDSS` automatically sets the value to `(167,167,1)`\n",
        "\n",
        " For an explanation of each of the arguments of `gamornet_predict_keras`, please have a look at the [API Documentation](https://gamornet.readthedocs.io/en/latest/api_docs.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUbDNlFCcWpR"
      },
      "source": [
        "## Understanding the Predictions Made By `Keras`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJMnlYiLb0Lq"
      },
      "outputs": [],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW11bh6Ucs05"
      },
      "source": [
        "As the above command shows, the returned predictions array is of the shape (# of images, 3) where for each image we have one single-dimensional array with 3 elements where\n",
        "\n",
        "* Element 0 is the probability for the image to be *disk-dominated*\n",
        "* Element 1 is the probability for the image to be *indeterminate*\n",
        "* Element 2 is the probability for the image to be *bulge-dominated*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arpjv6K7c_nj"
      },
      "outputs": [],
      "source": [
        "#1st image predictions \n",
        "print(predictions[0])\n",
        "\n",
        "#2nd image predictions\n",
        "print(predictions[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GttOA_yd0yI"
      },
      "source": [
        "As can be seen from the above arrays, GaMorNet predicts with 99.6% confidence that the first image is a disk-dominated galaxy and with 99.37% confidence that the second image is a bulge-dominated galaxy. \n",
        "\n",
        "These are indeed correct predictions as can be verified by investigating the light profiles of the galaxies in detail. \n",
        "\n",
        "*Tip: The number in the name of each image file refers to their SDSS Object ID*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwSechJjEf4"
      },
      "source": [
        "***\n",
        "\n",
        "#### Q - Reflect on the predictions you made earlier regarding applications for machine learning in astronomy. Where and why might it be advantageous to use neural networks to process astronomical data? Why, in particular, might it be useful in the context of galaxy studies?\n",
        "\n",
        "***\n",
        "\n",
        "Ans. Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7-kVTSijEf4"
      },
      "source": [
        "That's it for today's final tutorial on machine learning! Congrats on finishing up Part 5 and making it all the way through to the end of our Python in Astronomy workshop! 🎉"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yz6HXvLjEf4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}